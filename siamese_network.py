#!/usr/bin/env python3
# -*- coding: utf-8 -*-


from __future__ import absolute_import
from __future__ import division

import argparse
import logging
import sys
import time
import os
import pickle

import file2dict as fdt
import utils.read_minibatch as rmb
import utils.data_util as data_util

from datetime import datetime

import tensorflow as tf
import numpy as np

from AttributionModel import AttributionModel
from proj_rnn_cell import RNNCell
from proj_gru_cell import GRUCell

logger = logging.getLogger("RNN_Author_Attribution")
logger.setLevel(logging.DEBUG)
logging.basicConfig(format='%(message)s', level=logging.DEBUG)

class Config:
    """Holds model hyperparams and data information.

    The config class is used to store various hyperparameters and dataset
    information parameters. Model objects are passed a Config() object at
    instantiation.
    """

    cell_type="gru" # either rnn, gru or lstm

    window_size = 0

    word_num = 30
    max_length = 15 # longest length of a sentence we will process
    n_classes = 50 # in total, we have 50 classes
    dropout = 0.9

    embed_size = 50

    hidden_size = 100
    batch_size = 16

    n_epochs = 15
    regularization = 0.005

    max_grad_norm = 10.0 # max gradients norm for clipping
    lr = 0.001 # learning rate

    loss_lambda = 0.001
    threshold = 0.5

    def __init__(self, args):

        #self.cell = args.cell

        self.cell = GRUCell(Config.embed_size, Config.hidden_size)
        if "output_path" in args:
            # Where to save things.
            self.output_path = args.output_path
        else:
            self.output_path = "results/{}/{:%Y%m%d_%H%M%S}/".format("RNN", datetime.now())

        self.model_output = self.output_path + "model.weights"
        self.eval_output = self.output_path + "results.txt"

        #self.conll_output = self.output_path + "{}_predictions.conll".format(self.cell)
        self.log_output = self.output_path + "log"

class RNNModel():
    """
    Implements a recurrent neural network with an embedding layer and
    single hidden layer.
    """

    def add_placeholders(self):
        """Generates placeholder variables to represent the input tensors

        input_placeholder: Input placeholder tensor of  shape (None, self.max_length, n_features), type tf.int32
        labels_placeholder: Labels placeholder tensor of shape (None, self.max_length), type tf.int32
        mask_placeholder:  Mask placeholder tensor of shape (None, self.max_length), type tf.bool
        dropout_placeholder: Dropout value placeholder (scalar), type tf.float32

        """
        self.input_placeholder1 = tf.placeholder(tf.int32, [None, Config.max_length, Config.word_num])
        self.batch_mask_placeholder1 = tf.placeholder(tf.float32, [None, Config.max_length, Config.word_num])
        self.labels_placeholder1 = tf.placeholder(tf.int32, [None, Config.n_classes])
        self.mask_placeholder1 = tf.placeholder(tf.float32, [None, Config.max_length])

        self.input_placeholder2 = tf.placeholder(tf.int32, [None, Config.max_length, Config.word_num])
        self.batch_mask_placeholder2 = tf.placeholder(tf.float32, [None, Config.max_length, Config.word_num])
        self.labels_placeholder2 = tf.placeholder(tf.int32, [None, Config.n_classes])
        self.mask_placeholder2 = tf.placeholder(tf.float32, [None, Config.max_length])

        self.dropout_placeholder = tf.placeholder(tf.float32)

    def create_feed_dict(self, inputs_batch1, batch_feat_mask1, mask_batch1,
                        inputs_batch2, batch_feat_mask2, mask_batch2,
                         labels_batch1=None, labels_batch2=None, dropout=1):
        """Creates the feed_dict for the dependency parser.

        Args:
            inputs_batch: A batch of input data.
            mask_batch:   A batch of mask data.
            labels_batch: A batch of label data.
            dropout: The dropout rate.
        Returns:
            feed_dict: The feed dictionary mapping from placeholders to values.
        """

        feed_dict = {}
        feed_dict[self.batch_mask_placeholder1] = batch_feat_mask1
        if labels_batch1 != None:
            feed_dict[self.labels_placeholder1] = labels_batch1
        if inputs_batch1 != None:
            feed_dict[self.input_placeholder1] = inputs_batch1
        if mask_batch1 != None:
            feed_dict[self.mask_placeholder1] = mask_batch1

        feed_dict[self.batch_mask_placeholder2] = batch_feat_mask2
        if labels_batch1 != None:
            feed_dict[self.labels_placeholder2] = labels_batch2
        if inputs_batch1 != None:
            feed_dict[self.input_placeholder2] = inputs_batch2
        if mask_batch1 != None:
            feed_dict[self.mask_placeholder2] = mask_batch2

        feed_dict[self.dropout_placeholder] = dropout
        return feed_dict

    def add_embedding(self):
        """Adds an embedding layer that maps from input tokens (integers) to vectors and then
        concatenates those vectors:

        Returns:
            embeddings: tf.Tensor of shape (None, n_features*embed_size)
        """

        embeddingTensor = tf.Variable(self.pretrained_embeddings, tf.float32)

        # For input 1
        embeddingsTemp1 = tf.nn.embedding_lookup(embeddingTensor, self.input_placeholder1)
        mask_batch1 = tf.reshape(self.batch_mask_placeholder1, [-1, config.max_length, config.word_num, 1])
        mask_batch1 = tf.tile(mask_batch1, [1, 1, 1, config.embed_size])
        embeddings1 = tf.multiply(embeddingsTemp1, mask_batch1)
        embeddings1 = tf.reduce_sum(embeddings1, axis = 2)

        # For input 2
        embeddingsTemp2 = tf.nn.embedding_lookup(embeddingTensor, self.input_placeholder2)
        mask_batch2 = tf.reshape(self.batch_mask_placeholder2, [-1, config.max_length, config.word_num, 1])
        mask_batch2 = tf.tile(mask_batch2, [1, 1, 1, config.embed_size])
        embeddings2 = tf.multiply(embeddingsTemp2, mask_batch2)
        embeddings2 = tf.reduce_sum(embeddings2, axis = 2)

        return embeddings1, embeddings2

    def add_prediction_op(self):
        """Adds the unrolled RNN:
            h_0 = 0
            for t in 1 to T:
                o_t, h_t = cell(x_t, h_{t-1})
                o_drop_t = Dropout(o_t, dropout_rate)
                y_t = o_drop_t U + b_2

        Remember:
            * Use the xavier initilization for matrices.
            * Note that tf.nn.dropout takes the keep probability (1 - p_drop) as an argument.
            The keep probability should be set to the value of self.dropout_placeholder

        Returns:
            pred: tf.Tensor of shape (batch_size, max_length, n_classes)
        """

        x1, x2 = self.add_embedding()
        #x = self.input_placeholder

        dropout_rate = self.dropout_placeholder

        self.raw_preds1 = [] # Predicted output at each timestep should go here!
        self.raw_preds2 = []

        output1 = []
        output2 = []

        cell = GRUCell(Config.embed_size, Config.hidden_size)

        self.U = tf.get_variable('U',
                              [Config.hidden_size, Config.n_classes],
                              initializer = tf.contrib.layers.xavier_initializer())
        self.b2 = tf.get_variable('b2',
                              [Config.n_classes, ],
                              initializer = tf.contrib.layers.xavier_initializer())
        h1 = tf.zeros([tf.shape(x1)[0], Config.hidden_size])
        h2 = tf.zeros([tf.shape(x2)[0], Config.hidden_size])

        with tf.variable_scope("RNN"):
            # For input 1
            for time_step in range(config.max_length):
                if time_step >= 1:
                    tf.get_variable_scope().reuse_variables()
                o1, h1 = cell(x1[:,time_step,:], h1)
                output1.append(o1)
                o_drop1 = tf.nn.dropout(o1, dropout_rate)
                self.raw_preds1.append(tf.matmul(o_drop1, self.U) + self.b2)

            for time_step in range(config.max_length):
                if time_step >= 1:
                    tf.get_variable_scope().reuse_variables()
                o2, h2 = cell(x2[:,time_step,:], h2)
                output2.append(o2)
                o_drop2 = tf.nn.dropout(o2, dropout_rate)
                self.raw_preds2.append(tf.matmul(o_drop2, self.U) + self.b2)

        # Make sure to reshape raw_preds

        preds1=tf.pack(self.raw_preds1)
        preds1=tf.reshape(tf.transpose(preds1, [1, 0, 2]),[-1,Config.max_length,Config.n_classes])

        preds2=tf.pack(self.raw_preds2)
        preds2=tf.reshape(tf.transpose(preds2, [1, 0, 2]),[-1,Config.max_length,Config.n_classes])

        output1=tf.pack(output1)
        output1=tf.reshape(tf.transpose(output1, [1, 0, 2]),[-1,Config.max_length,Config.hidden_size])

        output2=tf.pack(output2)
        output2=tf.reshape(tf.transpose(output2, [1, 0, 2]),[-1,Config.max_length,Config.hidden_size])

        return preds1, preds2, output1, output2

    def extract_features(self, h1, h2):
        #extract features in the Siamese network
        feat_mask1=tf.reshape(self.mask_placeholder1,[-1,Config.max_length,1])
        feat_mask1=tf.tile(feat_mask1,[1,1,Config.hidden_size])

        feat_masked1=tf.multiply(h1,feat_mask1)
        feat_masked1=tf.reduce_sum(feat_masked1,axis=1)

        feat_mask2=tf.reshape(self.mask_placeholder2,[-1,Config.max_length,1])
        feat_mask2=tf.tile(feat_mask2,[1,1,Config.hidden_size])

        feat_masked2=tf.multiply(h2,feat_mask2)
        feat_masked2=tf.reduce_sum(feat_masked2,axis=1)

        return feat_masked1, feat_masked2

    def add_veri_prediction(self, feat1, feat2):
        cosine_pred = tf.reduce_sum(feat1 * feat2, 1) /( ( tf.sqrt(tf.reduce_sum(feat1 * feat1, 1)) * (tf.sqrt(tf.reduce_sum(feat2 * feat2, 1)) )) )

        return cosine_pred

    def add_loss_op(self, preds1, preds2, feat1, feat2):
        """Adds Ops for the loss function to the computational graph.

        Args:
            pred: A tensor of shape (batch_size, max_length, n_classes) containing the output of the neural
                  network before the softmax layer.
        Returns:
            loss: A 0-d tensor (scalar)
        """


        self.pred_mask1=tf.reshape(self.mask_placeholder1,[-1,Config.max_length,1])
        self.pred_mask1=tf.tile(self.pred_mask1,[1,1,Config.n_classes])

        self.pred_masked1=tf.multiply(preds1,self.pred_mask1)
        self.pred_masked1=tf.reduce_sum(self.pred_masked1,axis=1)

        self.pred_mask2=tf.reshape(self.mask_placeholder2,[-1,Config.max_length,1])
        self.pred_mask2=tf.tile(self.pred_mask2,[1,1,Config.n_classes])

        self.pred_masked2=tf.multiply(preds2,self.pred_mask2)
        self.pred_masked2=tf.reduce_sum(self.pred_masked2,axis=1)


        iden_loss1 = tf.nn.softmax_cross_entropy_with_logits(self.pred_masked1, self.labels_placeholder1)
        iden_loss2 = tf.nn.softmax_cross_entropy_with_logits(self.pred_masked2, self.labels_placeholder2)

        self.w_veri = tf.get_variable('w_veri', [1, ], initializer = tf.contrib.layers.xavier_initializer())
        self.b_veri = tf.get_variable('b_veri', [1, ], initializer = tf.contrib.layers.xavier_initializer())

        cosine_dis = tf.reduce_sum(feat1 * feat2, 1) / (tf.sqrt(tf.reduce_sum(feat1 * feat1, 1)) * (tf.sqrt(tf.reduce_sum(feat2 * feat2, 1)) ))
        veri_loss = tf.nn.sigmoid(tf.mul(self.w_veri, cosine_dis) + self.b_veri)

        # add identification loss
        loss = tf.reduce_mean(iden_loss1 + iden_loss2 + tf.mul(config.loss_lambda, veri_loss)) + config.regularization * ( tf.nn.l2_loss(self.U) )
        with tf.variable_scope("RNN/cell", reuse= True):
            # add regularization

            loss += config.regularization * (tf.nn.l2_loss(tf.get_variable("W_r"))
                                             + tf.nn.l2_loss(tf.get_variable("U_r"))
                                             + tf.nn.l2_loss(tf.get_variable("W_z"))
                                             + tf.nn.l2_loss(tf.get_variable("U_z"))
                                             + tf.nn.l2_loss(tf.get_variable("W_o"))
                                             + tf.nn.l2_loss(tf.get_variable("U_o")))
        return loss

    def add_training_op(self, loss):
        """Sets up the training Ops.

        Creates an optimizer and applies the gradients to all trainable variables.
        The Op returned by this function is what must be passed to the
        `sess.run()` call to cause the model to train. See

        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer

        for more information.

        Use tf.train.AdamOptimizer for this model.
        Calling optimizer.minimize() will return a train_op object.

        Args:
            loss: Loss tensor, from cross_entropy_loss.
        Returns:
            train_op: The Op for training.
        """

        train_op = tf.train.AdamOptimizer(Config.lr).minimize(loss)

        return train_op


    def train_on_batch(self, sess, inputs_batch1, batch_feat_mask1, labels_batch1, mask_batch1,
                       inputs_batch2, batch_feat_mask2, labels_batch2, mask_batch2):

        feed = self.create_feed_dict(inputs_batch1, batch_feat_mask1, mask_batch1,
                                     inputs_batch2, batch_feat_mask2, mask_batch2,
                                     labels_batch1, labels_batch2, dropout=Config.dropout)
        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)

        return loss

    def predict_on_batch(self, sess, inputs_batch1, batch_feat_mask1, mask_batch1,
                         inputs_batch2, batch_feat_mask2, mask_batch2):
        """Make predictions for the provided batch of data

        Args:
            sess: tf.Session()
            input_batch: np.ndarray of shape (n_samples, max_length)
            mask_batch: np.ndarray of shape (n_samples, max_length)
        Returns:
            predictions: np.ndarray of shape (n_samples, n_classes)
            (after softmax)
        """
        feed = self.create_feed_dict(inputs_batch1, batch_feat_mask1, mask_batch1,
                                     inputs_batch2, batch_feat_mask2, mask_batch2)
        cosine_pred = sess.run(self.cosine_pred, feed_dict=feed)
        return cosine_pred


        """
        record the history of the model
        this function will append the following content to the file (opened as f):

        ###
        training model:
        starting time:

        parameters：
            cell_type:
            embed_size:
            hidden_size:
            learning_rate:
            regularization:
            batch_size:

        training history:
        epoch       loss     train_accu      test_accu
        0           1.2      0.6             0.4
        ...

        """
    def record_history_init(self,f):
        f.write("###\n")
        f.write("training model: "+sys.argv[0]+"\n")
        f.write("starting time: "+str(datetime.now())+"\n")
        f.write("\n")
        f.write("parameters:\n")
        f.write("\tcell_type: "+Config.cell_type+"\n")
        f.write("\tembed_size: {}\n".format(Config.embed_size))
        f.write("\thidden_size: {}\n".format(Config.hidden_size))
        f.write("\tlearning_rate: {0:.4f}\n".format(Config.lr))
        f.write("\tregularization: {0:.5f}\n".format(Config.regularization))
        f.write("\tbatch_size: {}\n".format(Config.batch_size))
        f.write("\n")
        f.write("training history:\n")
        f.write("epoch \t\t loss \t\t train_accu \t\t test_accu\n")


        """
        write one line of training history
        """
    def record_history_accu(self,f,n_epoch,average_train_loss,train_accu,test_accu):
        f.write("{0:d} \t\t {1:.5f} \t\t {2:.5f} \t\t {3:.5f}\n".format(n_epoch,average_train_loss,train_accu,test_accu))


        """
        add two empty lines and close the file
        """
    def record_history_finish(self,f):
        f.write("END\n")
        f.write("\n")
        f.close()

    def test_model(self, session, batch_list):
        print "Now, testing on the test set..."
        total = 0
        accuCount = 0
        for batch in batch_list:
            batch_feat1 = np.array(batch[1][0], dtype = np.int32)[:, :, 0, :]
            batch_feat_mask1 = np.array(batch[1][0], dtype = np.float32)[:, :, 1, :]
            batch_mask1 = np.array(batch[2][0], dtype = np.float32)

            batch_feat2 = np.array(batch[1][1], dtype = np.int32)[:, :, 0, :]
            batch_feat_mask2 = np.array(batch[1][1], dtype = np.float32)[:, :, 1, :]
            batch_mask2 = np.array(batch[2][1], dtype = np.float32)

            raw_pred = self.predict_on_batch(session, batch_feat1, batch_feat_mask1, batch_mask1,
                                                    batch_feat2, batch_feat_mask2, batch_mask2)
            pred = np.array(raw_pred) > config.threshold
            true_label = np.array(batch[0][0]) == np.array(batch[0][1])

            accuCount += np.sum(pred == true_label)
            total += len(batch[0][0])
        accu = accuCount * 1.0 / total
        logger.info( ("Test accuracy %f" %(accu)) )
        return accu

    def test_trainset_model(self, session, batch_list):
        print "Now, testing on the trainig set, notice this is only for debugging..."
        total = 0
        accuCount = 0
        for batch in batch_list:
            batch_feat1 = np.array(batch[1][0], dtype = np.int32)[:, :, 0, :]
            batch_feat_mask1 = np.array(batch[1][0], dtype = np.float32)[:, :, 1, :]
            batch_mask1 = np.array(batch[2][0], dtype = np.float32)

            batch_feat2 = np.array(batch[1][1], dtype = np.int32)[:, :, 0, :]
            batch_feat_mask2 = np.array(batch[1][1], dtype = np.float32)[:, :, 1, :]
            batch_mask2 = np.array(batch[2][1], dtype = np.float32)

            raw_pred = self.predict_on_batch(session, batch_feat1, batch_feat_mask1, batch_mask1,
                                                    batch_feat2, batch_feat_mask2, batch_mask2)
            pred = np.array(raw_pred) > config.threshold
            true_label = np.array(batch[0][0]) == np.array(batch[0][1])

            accuCount += np.sum(pred == true_label)
            total += len(batch[0][0])
        accu = accuCount * 1.0 / total
        logger.info( ("Test accuracy on training set is: %f" %(accu)) )
        return accu

    def train_model(self):
        # modify txt name from here
        training_history_txt_filename='results/training_history_siamese_lambda_long.txt'

        if not os.path.exists(config.log_output):
            os.makedirs(os.path.dirname(config.log_output))
        handler = logging.FileHandler(config.log_output)
        handler.setLevel(logging.DEBUG)
        handler.setFormatter(logging.Formatter('%(message)s'))
        logging.getLogger().addHandler(handler)

        pkl_file = open('../data/batch_data/C50/data_sentence_pair.pkl', 'rb')
        batch_list = pickle.load(pkl_file)
        pkl_file.close()

        # write training_history
        training_history_file = open(training_history_txt_filename,'a+')
        self.record_history_init(training_history_file)

        test_size = int(len(batch_list) / 10)
        training_batch = batch_list[0 : len(batch_list) - test_size]
        print test_size, len(batch_list)
        testing_train_batch = batch_list[test_size : 2 * test_size]
        testing_batch = batch_list[len(batch_list) - test_size : len(batch_list)]

        init = tf.global_variables_initializer()
        saver = tf.train.Saver()
        with tf.Session() as session:
            session.run(init)
            #load_path = "results/RNN/20170310_1022/model.weights_20"
            #saver.restore(session, load_path)

            '''
            #the following is a test for what in tensor
            batch = training_batch[0]
            batch_label = rmb.convertOnehotLabel(batch[0],  Config.n_classes)
            batch_feat = np.array(batch[1], dtype = np.int32)[:, :, 0, :]
            batch_feat_mask = np.array(batch[1], dtype = np.float32)[:, :, 1, :]
            batch_mask = np.array(batch[2], dtype = np.float32)
            feed = self.create_feed_dict(batch_feat, batch_feat_mask, labels_batch=batch_label, mask_batch=batch_mask,
                                     dropout=Config.dropout)
            _, loss, raw_pred, pred = session.run([self.train_op, self.loss, self.raw_preds, self.pred], feed_dict=feed)
            ##############
            '''

            for iterTime in range(Config.n_epochs):
                loss_list = []
                smallIter = 0

                for batch in training_batch:
                    batch_label1 = rmb.convertOnehotLabel(batch[0][0],  Config.n_classes)
                    batch_feat1 = np.array(batch[1][0], dtype = np.int32)[:, :, 0, :]
                    batch_feat_mask1 = np.array(batch[1][0], dtype = np.float32)[:, :, 1, :]
                    batch_mask1 = np.array(batch[2][0], dtype = np.float32)

                    batch_label2 = rmb.convertOnehotLabel(batch[0][1],  Config.n_classes)
                    batch_feat2 = np.array(batch[1][1], dtype = np.int32)[:, :, 0, :]
                    batch_feat_mask2 = np.array(batch[1][1], dtype = np.float32)[:, :, 1, :]
                    batch_mask2 = np.array(batch[2][1], dtype = np.float32)

                    #print batch_mask
                    loss = self.train_on_batch(session, batch_feat1,batch_feat_mask1, batch_label1, batch_mask1,
                                               batch_feat2,batch_feat_mask2, batch_label2, batch_mask2)
                    loss_list.append(loss)
                    smallIter += 1

                    if(smallIter % 20 == 0):

                        #self.test_trainset_model(session, testing_train_batch)
                        #self.test_model(session, testing_batch)
                        logger.info(("Intermediate epoch %d Total Iteration %d: loss : %f" %(iterTime, smallIter, np.mean(np.mean(np.array(loss)))) ))

                # record training history on this epoch
                train_accu=self.test_trainset_model(session, testing_train_batch)
                test_accu=self.test_model(session, testing_batch)
                average_train_loss=np.mean(np.array(loss_list))
                self.record_history_accu(training_history_file,iterTime,average_train_loss,train_accu,test_accu)

                if(iterTime % 10 == 0):
                    logger.info(("epoch %d : loss : %f" %(iterTime, np.mean(np.mean(np.array(loss)))) ))
                    saver.save(session, self.config.model_output + "_%d"%(iterTime))

                    #if(smallIter % 200 == 0):
                    print ("Intermediate epoch %d : loss : %f" %(iterTime, np.mean(np.mean(np.array(loss)))) )

            print ("epoch %d : loss : %f" %(iterTime, np.mean(np.mean(np.array(loss)))) )

            self.record_history_finish(training_history_file)



    def __init__(self, config, pretrained_embeddings, report=None):

        #super(RNNModel, self).__init__(config)
        self.config = config

        self.pretrained_embeddings = pretrained_embeddings

        self.input_placeholder = None
        self.labels_placeholder = None
        self.batch_mask_placeholder = None
        self.mask_placeholder = None
        self.dropout_placeholder = None

        self.add_placeholders()
        self.pred1, self.pred2, self.output1, self.output2 = self.add_prediction_op()
        self.feat1, self.feat2 = self.extract_features(self.output1, self.output2)
        self.cosine_pred = self.add_veri_prediction(self.feat1, self.feat2)
        self.loss = self.add_loss_op(self.pred1, self.pred2, self.feat1, self.feat2)
        self.train_op = self.add_training_op(self.loss)


if __name__ == "__main__":
    args = "gru"
    config = Config(args)
    glove_path = "../data/glove/glove.6B.50d.txt"
    glove_vector = data_util.load_embeddings(glove_path, config.embed_size)
    model = RNNModel(config, glove_vector.astype(np.float32))
    model.train_model()
